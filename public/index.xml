<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Coded bioacoustics</title>
    <link>marce10.github.io/coded_bioacustics/</link>
    <description>Recent content on Coded bioacoustics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 28 May 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="marce10.github.io/coded_bioacustics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>dynaSpec: dynamic spectrograms in R</title>
      <link>marce10.github.io/coded_bioacustics/post/2020-05-28-dynaspec_dynamic_spectrograms_in_r/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2020-05-28-dynaspec_dynamic_spectrograms_in_r/</guid>
      <description>The R package dynaSpec can now be installed from github. This is a set of tools to generate dynamic spectrogram visualizations in video format. It is still on the making and new visualizations will be available soon. FFMPEG must be installed in order for this package to work.&#xA;To install dynaSpec from github you will need the R package devtools:&#xA;# From github devtools::install_github(&amp;#34;maRce10/dynaSpec&amp;#34;) #load package library(dynaSpec) Examples To run the following examples you will also need to load a few more packages:</description>
    </item>
    <item>
      <title>Automatic signal detection: a case study</title>
      <link>marce10.github.io/coded_bioacustics/post/2020-06-15-automatic_signal_detection_a_case_study/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2020-06-15-automatic_signal_detection_a_case_study/</guid>
      <description>Note that the R package ohun for optimizing sound event detection is now available and provides an improved version of the approaches shown in this post Some recent additions to warbleR aim to simplify the automatic detection of signals. The current post details these additions with a study case detecting inquiry calls of Spix&amp;rsquo;s disc-winged bats (Thyroptera tricolor).&#xA;Inquiry calls were recorded while the bats were flying in a flight cage.</description>
    </item>
    <item>
      <title>Interactive maps from Xeno-Canto recording localities in xc_maps()</title>
      <link>marce10.github.io/coded_bioacustics/post/2020-03-15-leaflet_xcmaps/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2020-03-15-leaflet_xcmaps/</guid>
      <description>The function xc_maps() from the R package warbleR let users visualize the geographic spread of recordings from Xeno-Canto downloaded through the quer_xc() function. However, the graphs are static, which becomes a bit painful when looking at many recordings. Fortunately the R package leaflet allows to create interactive maps in which users can zoom in at specific areas, check observation medatada and link observations to their Xeno-Canto webpage. This capability is now available in xc_maps().</description>
    </item>
    <item>
      <title>Acoustic space scatter plot</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-04-23-acoustic_space_scatter_plot/</link>
      <pubDate>Sun, 23 Apr 2017 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-04-23-acoustic_space_scatter_plot/</guid>
      <description>Some people have asked for the code we used to make figure 3 in the Methods in Ecology and Evolution paper describing warbleR. So, here it is. The figure was made in part by my collaborator Grace Smith-Vidaurre, so thanks to Grace for sharing.&#xA;The figure shows the grouping of long-billed hermit songs in the acoustic space based on similarity of dominant frequency contours. Similarity was assessed using dynamic time warping.</description>
    </item>
    <item>
      <title>Choosing the right method for measuring acoustic signal structure</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-02-17-choosing-the-right-method-for-measuring-acoustic-signal-structure/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-02-17-choosing-the-right-method-for-measuring-acoustic-signal-structure/</guid>
      <description>Bioacoustic research relies on quantifying the structure of acoustic` signals and comparing that structure across behavioral/ecological contexts, groups or species. However, measuring signal structure in a way that fully accounts for the variation in the signals could be a tricky task. Some of the differences that are apparent by visual inspection of spectrograms might not be picked up by some analyses. Hence, choosing the most appropriate analytical approach is a critical step.</description>
    </item>
    <item>
      <title>Signal detection with cross-correlation using monitoR </title>
      <link>marce10.github.io/coded_bioacustics/post/2016-12-15-detecting_signals_with_monitor/</link>
      <pubDate>Thu, 15 Dec 2016 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2016-12-15-detecting_signals_with_monitor/</guid>
      <description>Note that the R package ohun for optimizing sound event detection is now available and provides an improved version of the approaches shown in this post Here I show how to detect signals with cross-correlation using the very cool package monitoR. This package aims to facilitate acoustic template detection. The code here is similar but much less detailed than the quick start vignette of the monitoR package, so I encourage to look at the vignette if you want to learn more about it.</description>
    </item>
    <item>
      <title>About me</title>
      <link>marce10.github.io/coded_bioacustics/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/page/about/</guid>
      <description>I am broadly interested in the evolution of behavior from a evolutionary and cultural perspective, as well as the interplay between these two. I worked on hummingbird behavior during my PhD, and also explored the evolution of learned vocal signals in other groups using comparative analyses during my time at the Lab of O. I am currently working on social communication in communal roosting bats as postdoc at Universidad de Costa Rica.</description>
    </item>
    <item>
      <title>baRulho: an R package to quantify (animal) acoustic signal transmission and degradation</title>
      <link>marce10.github.io/coded_bioacustics/post/2020-02-13-barulho/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2020-02-13-barulho/</guid>
      <description>The baRulho package is intended to facilitate acoustic analysis of (animal) sound transmission experiments. Such studies typically aim to quantify changes in signal structure when transmitted in a given habitat by broadcasting and re-recording animal sounds at increasing distances. We will refer to these changes in signal structure &amp;lsquo;degradation&amp;rsquo; for the sake of simplicity. The package offers a workflow with functions to prepare the data set for analysis as well as to calculate and visualize several degradation metrics.</description>
    </item>
    <item>
      <title>Compare signals from selection tables to a set of templates using cross-correlation</title>
      <link>marce10.github.io/coded_bioacustics/post/2019-01-12-selection_table_vs_template_xcorr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2019-01-12-selection_table_vs_template_xcorr/</guid>
      <description>I got the following question about cross-correlation:&#xA;&amp;ldquo;We would like to compare every call within a selection table to a template of each owl, and get peak correlation coefficients on each call separately&amp;rdquo;&#xA;One way to do this would be putting the unidentified and template signals together into a single selection table, and then running cross-correlation. However, this will also compare all unidentified signals against each other, which can be very inefficient.</description>
    </item>
    <item>
      <title>Creating dynamic spectrograms (videos)</title>
      <link>marce10.github.io/coded_bioacustics/post/2016-12-12-create_dynamic_spectro_in_r/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2016-12-12-create_dynamic_spectro_in_r/</guid>
      <description>Note that the code below is now available in the new R package dynaSpec This code creates a video with a spectrogram scrolling from right to left. The spectrogram is synchronized with the audio. This is done by creating single image files for each of the movie frames and then putting them together in .mp4 video format. You will need the ffmpeg UNIX application to be able to run the code (only works for OSX and Linux).</description>
    </item>
    <item>
      <title>Creating song catalogs</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-03-17-creating_song_catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-03-17-creating_song_catalogs/</guid>
      <description>When looking at geographic variation of songs we usually want to compare the spectrograms from different individuals and sites. This can be challenging when working with large numbers of signals, individuals and/or sites. The new warbleR function catalog aims to simplify this task.&#xA;This is how it works:&#xA;The function plots a matrix of spectrograms from signals listed in a selection table (a data frame similar to the example data frame selec.</description>
    </item>
    <item>
      <title>Download a single recording for each species in a country from Xeno-Canto</title>
      <link>marce10.github.io/coded_bioacustics/post/2016-12-22-download_a_single_recording_for_each_species_in_a_site_from_xeno-canto/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2016-12-22-download_a_single_recording_for_each_species_in_a_site_from_xeno-canto/</guid>
      <description>A warbleR user asks if &amp;ldquo;there is any method for downloading from xeno canto a SINGLE individual of each species in Costa Rica&amp;rdquo;.&#xA;This can be done by 1) downloading the metadata of all recordings in a given site (in this case Costa Rica) using the querxc function from the package warbleR (which searches and downloads recordings from Xeno-Canto), 2) filtering the metadata to have only one recording per species, and 3) input the filtered metadata back into querxcto download the selected recordings.</description>
    </item>
    <item>
      <title>Editing wave objects from extended selection tables</title>
      <link>marce10.github.io/coded_bioacustics/post/2020-05-26-editing_wave_objects_from_extended_selection_tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2020-05-26-editing_wave_objects_from_extended_selection_tables/</guid>
      <description>&amp;ldquo;Is there a simple way to remove noise from the clips in an extended table &amp;ndash; I can do this by directly manipulating the attributes of the table but it seems a bit kludgy &amp;hellip; so again, am I missing something simple?&amp;rdquo;&#xA;Manipulating clips from extended selection tables can be pretty straightforward. It can be done by using lapply() to go over each clip. Things should be fine as long as you don&amp;rsquo;t mess with any time related feature (i.</description>
    </item>
    <item>
      <title>Evaluating group acoustic signatures using cross-correlation</title>
      <link>marce10.github.io/coded_bioacustics/post/2019-08-13-group_signature_xcorr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2019-08-13-group_signature_xcorr/</guid>
      <description>Social learning is often diagnosed by mapping the geographic variation of behavior. Behavioral variation at a small geographical scale that shows both sharp differences among localities and consistency within localities is indicative of social learning of local traditions. This pattern translates into a pretty straightforward statistical hypothesis: the behavior is more similar within than between groups (although absence of this pattern doesn&amp;rsquo;t necessarily imply a lack of learning!). In other words, if there is social learning going on, we can expect a group level signature.</description>
    </item>
    <item>
      <title>Extended selection tables</title>
      <link>marce10.github.io/coded_bioacustics/post/2018-05-15-extended_selection_tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2018-05-15-extended_selection_tables/</guid>
      <description>This post shows how to create and use the new warbleR object class extended_selection_table.&#xA;These objects are created with the selec_table() function. The function takes data frames containing selection data (sound file name, selection, start, end &amp;hellip;), checks whether the information is consistent (see checksels() function for details) and saves the &amp;lsquo;diagnostic&amp;rsquo; metadata as an attribute. When the argument extended = TRUE the function generates an object of class extended_selection_table which also contains a list of wave objects corresponding to each of the selections in the data frame.</description>
    </item>
    <item>
      <title>Fixing selections manually</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-08-03-fixing_selections_manually/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-08-03-fixing_selections_manually/</guid>
      <description>This short post shows how to use the seltailor function to adjust selection frequency and time &amp;lsquo;coordinates&amp;rsquo; in an interactive and iterative manner.&#xA;To be able to run the code you need warbleR 1.1.9 or higher, which hasn&amp;rsquo;t been released on CRAN and it&amp;rsquo;s only available on github. It can be installed using the devtools package as follows&#xA;# install devtools and monitor if are not yet installed # install devtools if is not yet installed if(!</description>
    </item>
    <item>
      <title>Frequency range detection from spectrum</title>
      <link>marce10.github.io/coded_bioacustics/post/2018-06-29-frequency_range_detection_from_spectrum/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2018-06-29-frequency_range_detection_from_spectrum/</guid>
      <description>We are often interested in getting the frequency range of acoustic signals, either because we have specific predictions about its variation or simply because we want to measure other stuff within that range. Measuring frequency range is typically done by drawing boxes in Raven/Avisoft/Syrinx. An alternative way, and potentially less subjective, is to infer the range from the energy distribution in the frequency domain applying amplitude thresholds on spectrums. I have added two new functions to warbleR that do exactly that:</description>
    </item>
    <item>
      <title>Individual sound files for each selection (or how to create a &lt;i&gt;warbleR&lt;/i&gt; function)</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-06-06-individual_sound_files_for_each_selection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-06-06-individual_sound_files_for_each_selection/</guid>
      <description>A friend of mine wants to &amp;ldquo;create individual sound files for each selection&amp;rdquo; in a selection table. This is a good opportunity to show how to create a function that works iteratively on signals in a selection table (like most warbleR functions).&#xA;It takes 3 main steps:&#xA;Create a (internal) function that does what we want on a single selection (i.e. a single row of the selection table)&#xA;Add and (X)lapply loop to run the function from step 1 iteratively on each row</description>
    </item>
    <item>
      <title>Pdf files with spectrograms of full recordings</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-01-07-create_pdf_files_with_spectrograms_of_full_recordings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-01-07-create_pdf_files_with_spectrograms_of_full_recordings/</guid>
      <description>The warbleR function lspec produces image files with spectrograms of whole recordings split into multiple rows. For a long recording several image files will be produced, which could be inconvenient when dealing with many long recordings. I recently added a new function lspec2pdf that combines lspec images in .jpeg format into a single pdf file (available in warbleR 1.1.5 or higher).&#xA;You need warbleR version 1.1.5 or higher to be able to run the code (currently you have to download it from github using the package devtools).</description>
    </item>
    <item>
      <title>Potential issues of the &#39;spectral parameters/PCA&#39; approach</title>
      <link>marce10.github.io/coded_bioacustics/post/2018-07-04-issues_spectral_parameters-pca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2018-07-04-issues_spectral_parameters-pca/</guid>
      <description>Somehow measuring a bunch of spectral/temporal parameters and then reducing its dimensionality using principal component analysis has become the standard procedure when looking at variation in signal structure (i.e. measuring acoustic space), particularly in behavioral ecology and comparative bioacoustics. In most cases the approach is used without any kind of ground-truthing that can help validate the analysis. Given the complexity of animal acoustic signals, the approach could miss key signal features.</description>
    </item>
    <item>
      <title>Rraven: Connecting R and Raven Sound Analysis Software</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-11-30-rraven_connecting_r_and_raven_sound_analysis_software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-11-30-rraven_connecting_r_and_raven_sound_analysis_software/</guid>
      <description>The Rraven package is designed to facilitate the exchange of data between R and Raven sound analysis software (Cornell Lab of Ornithology). Raven provides very powerful tools for the analysis of (animal) sounds. R can simplify the automatization of complex routines of analyses. Furthermore, R packages as warbleR, seewave and monitoR (among others) provide additional methods of analysis, working as a perfect complement for those found in Raven. Hence, bridging these applications can largely expand the bioacoustician&amp;rsquo;s toolkit.</description>
    </item>
    <item>
      <title>Signal detection with cross-correlation using warbleR</title>
      <link>marce10.github.io/coded_bioacustics/post/2020-04-10-signal_detection_warbler_vs_monitor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2020-04-10-signal_detection_warbler_vs_monitor/</guid>
      <description>Note that the R package ohun for optimizing sound event detection is now available and provides an improved version of the approaches shown in this post warbleR (v1.1.24) now includes functions to detect signals using cross-correlation similar to those in the package monitoR. There is already a blog post on cross-correlation detection using monitoR. In this post I show how to do that with warbleR and compare its performance against that from monitoR.</description>
    </item>
    <item>
      <title>Simulating animal vocalizations</title>
      <link>marce10.github.io/coded_bioacustics/post/2018-02-22-simulating_animal_vocalizations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2018-02-22-simulating_animal_vocalizations/</guid>
      <description>This post shows how to simulate animal vocalizations using the new warbleR function sim_songs. The function allows users to create song with several sub-units and harmonics, which are return as a wave object in the R environment. This can have several applications, from simulating song evolution to testing the efficacy of methods to measure acoustic structure for different signal types.&#xA;The function uses a brownian motion model of stochastic diffusion to simulate the changes in frequency (e.</description>
    </item>
    <item>
      <title>Song similarity using dynamic time warping</title>
      <link>marce10.github.io/coded_bioacustics/post/2016-09-12-similarity_of_acoustic_signals_with_dynamic_time_warping_dtw/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2016-09-12-similarity_of_acoustic_signals_with_dynamic_time_warping_dtw/</guid>
      <description>Here I show how to use the dfDTW function in warbleR to compare acoustics signals using dynamic time warping (DTW).&#xA;First load these packages (if not installed the code will install it):&#xA;x&amp;lt;-c(&amp;#34;vegan&amp;#34;, &amp;#34;warbleR&amp;#34;) A &amp;lt;- lapply(x, function(y) { if(!y %in% installed.packages()[,&amp;#34;Package&amp;#34;]) install.packages(y) require(y, character.only = T) }) and load example data from warbleR&#xA;# optional, save it in a temporal folder # setwd(tempdir()) data(list = c( &amp;#34;Phae.long1&amp;#34;, &amp;#34;Phae.long2&amp;#34;,&amp;#34;Phae.long3&amp;#34;, &amp;#34;Phae.long4&amp;#34;,&amp;#34;selec.table&amp;#34;)) writeWave(Phae.</description>
    </item>
    <item>
      <title>Spectrograms on trees</title>
      <link>marce10.github.io/coded_bioacustics/post/2019-01-12-phylo_spectro_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2019-01-12-phylo_spectro_function/</guid>
      <description>This post describes the new warbleR function phylo_spectro. The function adds spectrograms of sounds annotated in a selection table (&amp;lsquo;X argument) onto the tips of a tree (of class &amp;lsquo;phylo&amp;rsquo;). The &amp;rsquo;tip.label&amp;rsquo; column in &amp;lsquo;X&amp;rsquo; is used to match spectrograms and tree tips. The function uses internally the plot.phylo function from the ape package to plot the tree and warbleR&amp;rsquo;s specreator function to create the spectrograms. Arguments for both of these functions can be provided for further customization.</description>
    </item>
    <item>
      <title>Updates on catalog function</title>
      <link>marce10.github.io/coded_bioacustics/post/2017-07-31-updates_on_catalog_function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2017-07-31-updates_on_catalog_function/</guid>
      <description>A previous post described the new function catalog. Here are a few updates on catalog based on suggestions from warbleR users.&#xA;To be able to run the code you need warbleR 1.1.9 or higher, which hasn&amp;rsquo;t been released on CRAN and it&amp;rsquo;s only available in the most recent development version on github. It can be installed using the devtools package as follows&#xA;# install devtools if is not yet installed if(!</description>
    </item>
    <item>
      <title>Using your own frequency contours on DTW</title>
      <link>marce10.github.io/coded_bioacustics/post/2019-01-11-custom_contours_for_dtw/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2019-01-11-custom_contours_for_dtw/</guid>
      <description>I got the following question about dynamic time warping on frequency contours:&#xA;&amp;ldquo;what I am looking for is to use ffDTW on a file in which I have a column for the filename and then 20 pitch measurements for each of 10000 files (e.g. 10000 rows). Do you have suggestions?&amp;rdquo;&#xA;There is a workaround in warbleR to do that:&#xA;The function dfDTW() has the argument ts.df (for time series data frame) that allows to input your own frequency contours (or any other sequences of values taken along the signals).</description>
    </item>
    <item>
      <title>Working with higher structural levels in vocal signals</title>
      <link>marce10.github.io/coded_bioacustics/post/2019-02-16-working_with_songs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>marce10.github.io/coded_bioacustics/post/2019-02-16-working_with_songs/</guid>
      <description>Animal vocalizations can be hierarchically structured: elements group together in syllables, syllables in songs, songs in bouts and so on. Many important biological patterns of vocal variation are better described at higher structural levels, so we are often interested in characterizing vocalizations at those levels. There are several tools in warbleR to explore and measure features above the element level. For simplicity, any level above &amp;rsquo;elements&amp;rsquo; will be refered to as &amp;lsquo;songs&amp;rsquo; in this post as well as in the warbleR functions described here.</description>
    </item>
  </channel>
</rss>
